\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\citation{rudolph_trainability_2024}
\citation{rudolph_trainability_2024}
\citation{rudolph_generation_2022}
\citation{schollwock_density-matrix_2011}
\citation{mangini_low-variance_2024}
\citation{rudolph_trainability_2024}
\citation{collura_tensor_2024}
\citation{garcia-perez_learning_2021}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to Quantum Computing}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Introduction to Tensor Network}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Introduction to Machine Learning and Generative Models}{2}{section.1.3}\protected@file@percent }
\citation{haghshenas_variational_2022}
\citation{han_unsupervised_2018}
\citation{schollwock_density-matrix_2011}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Theoretical Background}{3}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Description of generative model}{3}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Quantum Circuit Born Machine}{3}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Tensor-Network Born Machine}{3}{subsection.2.1.2}\protected@file@percent }
\citation{rudolph_trainability_2024}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Explicit and Implicit Loss Functions}{4}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Focus on Maximum Mean Discrepancy}{4}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Efficient loss calculation with tensor network}{5}{section.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Tensor network representation of the sampling probabilities $p(b_1b_2\ldots  b_n)$. Each node corresponds to a local projector $\Pi _{b_i}$, and the open legs represent the outcomes $b_i$.\relax }}{6}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:probs_tn}{{2.1}{6}{Tensor network representation of the sampling probabilities $p(b_1b_2\ldots b_n)$. Each node corresponds to a local projector $\Pi _{b_i}$, and the open legs represent the outcomes $b_i$.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Tensor network representation of the first term in the MMD expression. The kernel matrix $ K $ acts on the output space indexed by $ x $ and $ y $, and can be realized as a kernel MPO.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{fig:overlap_tn}{{2.2}{7}{Tensor network representation of the first term in the MMD expression. The kernel matrix $ K $ acts on the output space indexed by $ x $ and $ y $, and can be realized as a kernel MPO.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The data bitstrings are represented as computational MPS, and the hyperindex allows efficient summation over all possible bitstrings.\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:data_tn}{{2.3}{7}{The data bitstrings are represented as computational MPS, and the hyperindex allows efficient summation over all possible bitstrings.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Appendix: other measures}{7}{subsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MMD as MPO as sum of operators of different bodyness}{7}{section*.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Tensor network representation of the mixed term in the MMD expression. The kernel matrix $ K $ acts on the output space indexed by $ x $ and $ y $, and can be realized as a kernel MPO.\relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:overlap_data_tn}{{2.4}{8}{Tensor network representation of the mixed term in the MMD expression. The kernel matrix $ K $ acts on the output space indexed by $ x $ and $ y $, and can be realized as a kernel MPO.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces We contract the model (in blue) and the data (green) with the MPO representing the loss function (organge). - figure to bi fixed, I didn't noticed the legend was not rendered\relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig:example}{{2.5}{8}{We contract the model (in blue) and the data (green) with the MPO representing the loss function (organge). - figure to bi fixed, I didn't noticed the legend was not rendered\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{Local quantum fidelity}{8}{section*.10}\protected@file@percent }
\citation{rudolph_trainability_2024}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Training}{10}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Training}{10}{section.3.1}\protected@file@percent }
\citation{martin_barren_2023}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Training results and corresponding generated samples for MMD-based training.\relax }}{11}{figure.caption.11}\protected@file@percent }
\newlabel{fig:training_mmd}{{3.1}{11}{Training results and corresponding generated samples for MMD-based training.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Barren Plateau Analysis}{11}{section.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Training results and corresponding generated samples for DKL-based training.\relax }}{12}{figure.caption.12}\protected@file@percent }
\newlabel{fig:training_dkl}{{3.2}{12}{Training results and corresponding generated samples for DKL-based training.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Variance of the loss function at random initialization as a function of the number of qubits, for different bond dimensions. The exponential decay suggests the presence of a barren plateau.\relax }}{13}{figure.caption.13}\protected@file@percent }
\newlabel{fig:barren_plateau}{{3.3}{13}{Variance of the loss function at random initialization as a function of the number of qubits, for different bond dimensions. The exponential decay suggests the presence of a barren plateau.\relax }{figure.caption.13}{}}
\bibdata{bibliography/thesis}
\bibstyle{ieeetr}
\gdef \@abspage@last{20}
