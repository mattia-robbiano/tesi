
@article{jerbi_shadows_2024,
	title = {Shadows of quantum machine learning},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-49877-8},
	doi = {10.1038/s41467-024-49877-8},
	abstract = {Quantum machine learning is often highlighted as one of the most promising practical applications for which quantum computers could provide a computational advantage. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we introduce a class of quantum models where quantum resources are only required during training, while the deployment of the trained model is classical. Specifically, the training phase of our models ends with the generation of a ‚Äòshadow model‚Äô from which the classical deployment becomes possible. We prove that: (i) this class of models is universal for classically-deployed quantum machine learning; (ii) it does have restricted learning capacities compared to ‚Äòfully quantum‚Äô models, but nonetheless (iii) it achieves a provable learning advantage over fully classical learners, contingent on widely believed assumptions in complexity theory. These results provide compelling evidence that quantum machine learning can confer learning advantages across a substantially broader range of scenarios, where quantum computers are exclusively employed during the training phase. By enabling classical deployment, our approach facilitates the implementation of quantum machine learning models in various practical contexts.},
	language = {en},
	number = {1},
	urldate = {2024-10-29},
	journal = {Nat Commun},
	author = {Jerbi, Sofiene and Gyurik, Casper and Marshall, Simon C. and Molteni, Riccardo and Dunjko, Vedran},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information theory and computation, Quantum information},
	pages = {5676},
}

@misc{rudolph_generation_2022,
	title = {Generation of {High}-{Resolution} {Handwritten} {Digits} with an {Ion}-{Trap} {Quantum} {Computer}},
	url = {http://arxiv.org/abs/2012.03924},
	doi = {10.48550/arXiv.2012.03924},
	abstract = {Generating high-quality data (e.g. images or video) is one of the most exciting and challenging frontiers in unsupervised machine learning. Utilizing quantum computers in such tasks to potentially enhance conventional machine learning algorithms has emerged as a promising application, but poses big challenges due to the limited number of qubits and the level of gate noise in available devices. In this work, we provide the first practical and experimental implementation of a quantum-classical generative algorithm capable of generating high-resolution images of handwritten digits with state-of-the-art gate-based quantum computers. In our quantum-assisted machine learning framework, we implement a quantum-circuit based generative model to learn and sample the prior distribution of a Generative Adversarial Network. We introduce a multi-basis technique that leverages the unique possibility of measuring quantum states in different bases, hence enhancing the expressivity of the prior distribution. We train this hybrid algorithm on an ion-trap device based on \${\textasciicircum}\{171\}\$Yb\${\textasciicircum}\{+\}\$ ion qubits to generate high-quality images and quantitatively outperform comparable classical Generative Adversarial Networks trained on the popular MNIST data set for handwritten digits.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Rudolph, Manuel S. and Toussaint, Ntwali Bashige and Katabarwa, Amara and Johri, Sonika and Peropadre, Borja and Perdomo-Ortiz, Alejandro},
	month = jun,
	year = {2022},
	note = {arXiv:2012.03924},
	keywords = {Quantum Physics},
}

@misc{mangini_low-variance_2024,
	title = {Low-variance observable estimation with informationally-complete measurements and tensor networks},
	url = {http://arxiv.org/abs/2407.02923},
	doi = {10.48550/arXiv.2407.02923},
	abstract = {We propose a method for providing unbiased estimators of multiple observables with low statistical error by utilizing informationally (over)complete measurements and tensor networks. The technique consists of an observable-specific classical optimization of the measurement data based on tensor networks leading to low-variance estimations. Compared to other observable estimation protocols based on classical shadows and measurement frames, our approach offers several advantages: (i) it can be optimized to provide lower statistical error, resulting in a reduced measurement budget to achieve a specified estimation precision; (ii) it scales to a large number of qubits due to the tensor network structure; (iii) it can be applied to any measurement protocol with measurement operators that have an efficient representation in terms of tensor networks. We benchmark the method through various numerical examples, including spin and chemical systems in both infinite and finite statistics scenarios, and show how optimal estimation can be found even when we use tensor networks with low bond dimensions.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Mangini, Stefano and Cavalcanti, Daniel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02923 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{garcia-perez_learning_2021,
	title = {Learning to {Measure}: {Adaptive} {Informationally} {Complete} {Generalized} {Measurements} for {Quantum} {Algorithms}},
	volume = {2},
	issn = {2691-3399},
	shorttitle = {Learning to {Measure}},
	url = {http://arxiv.org/abs/2104.00569},
	doi = {10.1103/PRXQuantum.2.040342},
	abstract = {Many prominent quantum computing algorithms with applications in fields such as chemistry and materials science require a large number of measurements, which represents an important roadblock for future real-world use cases. We introduce a novel approach to tackle this problem through an adaptive measurement scheme. We present an algorithm that optimizes informationally complete positive operator-valued measurements (POVMs) on the fly in order to minimize the statistical fluctuations in the estimation of relevant cost functions. We show its advantage by improving the efficiency of the variational quantum eigensolver in calculating ground-state energies of molecular Hamiltonians with extensive numerical simulations. Our results indicate that the proposed method is competitive with state-of-the-art measurement-reduction approaches in terms of efficiency. In addition, the informational completeness of the approach offers a crucial advantage, as the measurement data can be reused to infer other quantities of interest. We demonstrate the feasibility of this prospect by reusing ground-state energy-estimation data to perform high-fidelity reduced state tomography.},
	number = {4},
	urldate = {2025-01-27},
	journal = {PRX Quantum},
	author = {Garc√≠a-P√©rez, Guillermo and Rossi, Matteo A. C. and Sokolov, Boris and Tacchino, Francesco and Barkoutsos, Panagiotis Kl and Mazzola, Guglielmo and Tavernelli, Ivano and Maniscalco, Sabrina},
	month = nov,
	year = {2021},
	note = {arXiv:2104.00569 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {040342},
}

@article{han_unsupervised_2018,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012},
	doi = {10.1103/PhysRevX.8.031012},
	abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
	number = {3},
	urldate = {2025-01-24},
	journal = {Phys. Rev. X},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = jul,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {031012},
}

@article{cheng_tree_2019,
	title = {Tree tensor networks for generative modeling},
	volume = {99},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.99.155131},
	doi = {10.1103/PhysRevB.99.155131},
	language = {en},
	number = {15},
	urldate = {2025-01-20},
	journal = {Phys. Rev. B},
	author = {Cheng, Song and Wang, Lei and Xiang, Tao and Zhang, Pan},
	month = apr,
	year = {2019},
	pages = {155131},
}

@misc{meiburg_generative_2024,
	title = {Generative {Learning} of {Continuous} {Data} by {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2310.20498},
	doi = {10.48550/arXiv.2310.20498},
	abstract = {Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Meiburg, Alex and Chen, Jing and Miller, Jacob and Tihon, Rapha√´lle and Rabusseau, Guillaume and Perdomo-Ortiz, Alejandro},
	month = jul,
	year = {2024},
	note = {arXiv:2310.20498 [cs]},
	keywords = {Computer Science - Machine Learning, Quantum Physics, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics},
}

@article{rudolph_trainability_2024,
	title = {Trainability barriers and opportunities in quantum generative modeling},
	volume = {10},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-024-00902-0},
	doi = {10.1038/s41534-024-00902-0},
	abstract = {Abstract
            Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {npj Quantum Inf},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zo√´},
	month = nov,
	year = {2024},
	pages = {116},
}

@inproceedings{glasser_expressive_2019,
	title = {Expressive power of tensor-network factorizations for probabilistic modeling},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
	abstract = {Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.},
	urldate = {2025-03-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
	year = {2019},
}

@misc{ben-dov_regularized_2025,
	title = {Regularized second-order optimization of tensor-network {Born} machines},
	url = {http://arxiv.org/abs/2501.18691},
	doi = {10.48550/arXiv.2501.18691},
	abstract = {Tensor-network Born machines (TNBMs) are quantum-inspired generative models for learning data distributions. Using tensor-network contraction and optimization techniques, the model learns an efficient representation of the target distribution, capable of capturing complex correlations with a compact parameterization. Despite their promise, the optimization of TNBMs presents several challenges. A key bottleneck of TNBMs is the logarithmic nature of the loss function that is commonly used for this problem. The single-tensor logarithmic optimization problem cannot be solved analytically, necessitating an iterative approach that slows down convergence and increases the risk of getting trapped in one of many non-optimal local minima. In this paper, we present an improved second-order optimization technique for TNBM training, which significantly enhances convergence rates and the quality of the optimized model. Our method employs a modified Newton's method on the manifold of normalized states, incorporating regularization of the loss landscape to mitigate local minima issues. We demonstrate the effectiveness of our approach by training a one-dimensional matrix product state (MPS) on both discrete and continuous datasets, showcasing its advantages in terms of stability, efficiency, and generalization.},
	language = {en},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {Ben-Dov, Matan and Chen, Jing},
	month = jan,
	year = {2025},
	note = {arXiv:2501.18691 [cs]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
}

@article{martin_barren_2023,
	title = {Barren plateaus in quantum tensor network optimization},
	volume = {7},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2209.00292},
	doi = {10.22331/q-2023-04-13-974},
	abstract = {We analyze the barren plateau phenomenon in the variational optimization of quantum circuits inspired by matrix product states (qMPS), tree tensor networks (qTTN), and the multiscale entanglement renormalization ansatz (qMERA). We consider as the cost function the expectation value of a Hamiltonian that is a sum of local terms. For randomly chosen variational parameters we show that the variance of the cost function gradient decreases exponentially with the distance of a Hamiltonian term from the canonical centre in the quantum tensor network. Therefore, as a function of qubit count, for qMPS most gradient variances decrease exponentially and for qTTN as well as qMERA they decrease polynomially. We also show that the calculation of these gradients is exponentially more efficient on a classical computer than on a quantum computer.},
	urldate = {2025-03-24},
	journal = {Quantum},
	author = {Mart√≠n, Enrique Cervero and Plekhanov, Kirill and Lubasch, Michael},
	month = apr,
	year = {2023},
	note = {arXiv:2209.00292 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {974},
}

@article{levy_classical_2024,
	title = {Classical shadows for quantum process tomography on near-term quantum computers},
	volume = {6},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.013029},
	doi = {10.1103/PhysRevResearch.6.013029},
	abstract = {Quantum process tomography is a powerful tool for understanding quantum channels and characterizing the properties of quantum devices. Inspired by recent advances using classical shadows in quantum state tomography [H.-Y. Huang, R. Kueng, and J. Preskill, Nat. Phys. 16, 1050 (2020).], we have developed ShadowQPT, a classical shadow method for quantum process tomography. We introduce two related formulations with and without ancilla qubits. ShadowQPT stochastically reconstructs the Choi matrix of the device allowing for an a posteri classical evaluation of the device on arbitrary inputs with respect to arbitrary outputs. Using shadows, we then show how to compute overlaps, generate all ùëò-weight reduced processes, and perform reconstruction via Hamiltonian learning. These latter two tasks are efficient for large systems as the number of quantum measurements needed scales only logarithmically with the number of qubits. A number of additional approximations and improvements are developed, including the use of a pair-factorized Clifford shadow and a series of postprocessing techniques that significantly enhance the accuracy for recovering the quantum channel. We have implemented ShadowQPT using both Pauli and Clifford measurements on the IonQ trapped ion quantum computer for quantum processes up to ùëõ=4 qubits, and we achieved good performance.},
	number = {1},
	urldate = {2025-04-13},
	journal = {Phys. Rev. Res.},
	author = {Levy, Ryan and Luo, Di and Clark, Bryan K.},
	month = jan,
	year = {2024},
	note = {Publisher: American Physical Society},
	pages = {013029},
}

@article{hinton_learning_1986,
	title = {Learning and relearning in {Boltzmann} machines},
	volume = {1},
	journal = {Parallel Distributed Processing},
	author = {Hinton, G. and Sejnowski, Terrence},
	month = jan,
	year = {1986},
}

@article{liu_differentiable_2018,
	title = {Differentiable learning of quantum circuit {Born} machines},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevA.98.062324},
	doi = {10.1103/PhysRevA.98.062324},
	abstract = {Quantum circuit Born machines are generative models which represent the probability distribution of classical dataset as quantum pure states. Computational complexity considerations of the quantum sampling problem suggest that the quantum circuits exhibit stronger expressibility compared to classical neural networks. One can efficiently draw samples from the quantum circuits via projective measurements on qubits. However, similar to the leading implicit generative models in deep learning, such as the generative adversarial networks, the quantum circuits cannot provide the likelihood of the generated samples, which poses a challenge to the training. We devise an efficient gradient-based learning algorithm for the quantum circuit Born machine by minimizing the kerneled maximum mean discrepancy loss. We simulated generative modeling of the BARS-AND-STRIPES dataset and Gaussian mixture distributions using deep quantum circuits. Our experiments show the importance of circuit depth and the gradient-based optimization algorithm. The proposed learning algorithm is runnable on near-term quantum device and can exhibit quantum advantages for probabilistic generative modeling.},
	number = {6},
	urldate = {2025-04-15},
	journal = {Phys. Rev. A},
	author = {Liu, Jin-Guo and Wang, Lei},
	month = dec,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {062324},
}

@article{perdomo-ortiz_opportunities_2018,
	title = {Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers},
	volume = {3},
	issn = {2058-9565},
	url = {https://dx.doi.org/10.1088/2058-9565/aab859},
	doi = {10.1088/2058-9565/aab859},
	abstract = {With quantum computing technologies nearing the era of commercialization and quantum supremacy, machine learning (ML) appears as one of the promising ‚Äòkiller‚Äô applications. Despite significant effort, there has been a disconnect between most quantum ML proposals, the needs of ML practitioners, and the capabilities of near-term quantum devices to demonstrate quantum enhancement in the near future. In this contribution to the focus collection ‚ÄòWhat would you do with 1000 qubits?‚Äô, we provide concrete examples of intractable ML tasks that could be enhanced with near-term devices. We argue that to reach this target, the focus should be on areas where ML researchers are struggling, such as generative models in unsupervised and semi-supervised learning, instead of the popular and more tractable supervised learning techniques. We also highlight the case of classical datasets with potential quantum-like statistical correlations where quantum models could be more suitable. We focus on hybrid quantum‚Äìclassical approaches and illustrate some of the key challenges we foresee for near-term implementations. Finally, we introduce the quantum-assisted Helmholtz machine (QAHM), an attempt to use near-term quantum devices to tackle high-dimensional datasets of continuous variables. Instead of using quantum computers to assist deep learning, as previous approaches do, the QAHM uses deep learning to extract a low-dimensional binary representation of data, suitable for relatively small quantum processors which can assist the training of an unsupervised generative model. Although we illustrate this concept on a quantum annealer, other quantum platforms could benefit as well from this hybrid quantum‚Äìclassical framework.},
	language = {en},
	number = {3},
	urldate = {2025-04-15},
	journal = {Quantum Sci. Technol.},
	author = {Perdomo-Ortiz, Alejandro and Benedetti, Marcello and Realpe-G√≥mez, John and Biswas, Rupak},
	month = jun,
	year = {2018},
	note = {Publisher: IOP Publishing},
	pages = {030502},
}

@misc{noauthor_born_nodate,
	title = {The {Born} supremacy: quantum advantage and training of an {Ising} {Born} machine {\textbar} npj {Quantum} {Information}},
	url = {https://www.nature.com/articles/s41534-020-00288-9},
	urldate = {2025-04-15},
}

@article{sweke_quantum_2021,
	title = {On the {Quantum} versus {Classical} {Learnability} of {Discrete} {Distributions}},
	volume = {5},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2007.14451},
	doi = {10.22331/q-2021-03-23-417},
	abstract = {Here we study the comparative power of classical and quantum learners for generative modelling within the Probably Approximately Correct (PAC) framework. More specifically we consider the following task: Given samples from some unknown discrete probability distribution, output with high probability an efficient algorithm for generating new samples from a good approximation of the original distribution. Our primary result is the explicit construction of a class of discrete probability distributions which, under the decisional Diffie-Hellman assumption, is provably not efficiently PAC learnable by a classical generative modelling algorithm, but for which we construct an efficient quantum learner. This class of distributions therefore provides a concrete example of a generative modelling problem for which quantum learners exhibit a provable advantage over classical learning algorithms. In addition, we discuss techniques for proving classical generative modelling hardness results, as well as the relationship between the PAC learnability of Boolean functions and the PAC learnability of discrete probability distributions.},
	urldate = {2025-04-15},
	journal = {Quantum},
	author = {Sweke, Ryan and Seifert, Jean-Pierre and Hangleiter, Dominik and Eisert, Jens},
	month = mar,
	year = {2021},
	note = {arXiv:2007.14451 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	pages = {417},
}

@article{gao_enhancing_2022,
	title = {Enhancing {Generative} {Models} via {Quantum} {Correlations}},
	volume = {12},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.12.021037},
	doi = {10.1103/PhysRevX.12.021037},
	abstract = {Generative modeling using samples drawn from the probability distribution constitutes a powerful approach for unsupervised machine learning. Quantum mechanical systems can produce probability distributions that exhibit quantum correlations which are difficult to capture using classical models. We show theoretically that such quantum-inspired correlations provide a powerful resource for generative modeling. In particular, we provide an unconditional proof of separation in expressive power between a class of widely used generative models, known as Bayesian networks, and its minimal quantum-inspired extension. We show that this expressivity enhancement is associated with quantum nonlocality and quantum contextuality. Furthermore, we numerically test this separation on standard machine-learning data sets and show that it holds for practical problems. The possibility of quantum-inspired enhancement demonstrated in this work not only sheds light on the design of useful quantum machine-learning protocols but also provides inspiration to draw on ideas from quantum foundations to improve purely classical algorithms.},
	number = {2},
	urldate = {2025-04-15},
	journal = {Phys. Rev. X},
	author = {Gao, Xun and Anschuetz, Eric R. and Wang, Sheng-Tao and Cirac, J. Ignacio and Lukin, Mikhail D.},
	month = may,
	year = {2022},
	note = {Publisher: American Physical Society},
	pages = {021037},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised quantum circuit learning in high energy physics {\textbar} {Phys}. {Rev}. {D}},
	url = {https://journals.aps.org/prd/abstract/10.1103/PhysRevD.106.096006},
	urldate = {2025-04-15},
}

@misc{puljak_tn4ml_2025,
	title = {tn4ml: {Tensor} {Network} {Training} and {Customization} for {Machine} {Learning}},
	shorttitle = {tn4ml},
	url = {http://arxiv.org/abs/2502.13090},
	doi = {10.48550/arXiv.2502.13090},
	abstract = {Tensor Networks have emerged as a prominent alternative to neural networks for addressing Machine Learning challenges in foundational sciences, paving the way for their applications to real-life problems. This paper introduces tn4ml, a novel library designed to seamlessly integrate Tensor Networks into optimization pipelines for Machine Learning tasks. Inspired by existing Machine Learning frameworks, the library offers a user-friendly structure with modules for data embedding, objective function definition, and model training using diverse optimization strategies. We demonstrate its versatility through two examples: supervised learning on tabular data and unsupervised learning on an image dataset. Additionally, we analyze how customizing the parts of the Machine Learning pipeline for Tensor Networks influences performance metrics.},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Puljak, Ema and Sanchez-Ramirez, Sergio and Masot-Llima, Sergi and Vall√®s-Muns, Jofre and Garcia-Saez, Artur and Pierini, Maurizio},
	month = feb,
	year = {2025},
	note = {arXiv:2502.13090 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Quantum Physics},
}

@misc{rudolph_generation_2022,
	title = {Generation of {High}-{Resolution} {Handwritten} {Digits} with an {Ion}-{Trap} {Quantum} {Computer}},
	url = {http://arxiv.org/abs/2012.03924},
	doi = {10.48550/arXiv.2012.03924},
	abstract = {Generating high-quality data (e.g. images or video) is one of the most exciting and challenging frontiers in unsupervised machine learning. Utilizing quantum computers in such tasks to potentially enhance conventional machine learning algorithms has emerged as a promising application, but poses big challenges due to the limited number of qubits and the level of gate noise in available devices. In this work, we provide the first practical and experimental implementation of a quantum-classical generative algorithm capable of generating high-resolution images of handwritten digits with state-of-the-art gate-based quantum computers. In our quantum-assisted machine learning framework, we implement a quantum-circuit based generative model to learn and sample the prior distribution of a Generative Adversarial Network. We introduce a multi-basis technique that leverages the unique possibility of measuring quantum states in different bases, hence enhancing the expressivity of the prior distribution. We train this hybrid algorithm on an ion-trap device based on \${\textasciicircum}\{171\}\$Yb\${\textasciicircum}\{+\}\$ ion qubits to generate high-quality images and quantitatively outperform comparable classical Generative Adversarial Networks trained on the popular MNIST data set for handwritten digits.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Rudolph, Manuel S. and Toussaint, Ntwali Bashige and Katabarwa, Amara and Johri, Sonika and Peropadre, Borja and Perdomo-Ortiz, Alejandro},
	month = jun,
	year = {2022},
	note = {arXiv:2012.03924},
	keywords = {Quantum Physics},
}

@article{schollwock_density-matrix_2011,
	series = {January 2011 {Special} {Issue}},
	title = {The density-matrix renormalization group in the age of matrix product states},
	volume = {326},
	issn = {0003-4916},
	url = {https://www.sciencedirect.com/science/article/pii/S0003491610001752},
	doi = {10.1016/j.aop.2010.09.012},
	abstract = {The density-matrix renormalization group method (DMRG) has established itself over the last decade as the leading method for the simulation of the statics and dynamics of one-dimensional strongly correlated quantum lattice systems. In the further development of the method, the realization that DMRG operates on a highly interesting class of quantum states, so-called matrix product states (MPS), has allowed a much deeper understanding of the inner structure of the DMRG method, its further potential and its limitations. In this paper, I want to give a detailed exposition of current DMRG thinking in the MPS language in order to make the advisable implementation of the family of DMRG algorithms in exclusively MPS terms transparent. I then move on to discuss some directions of potentially fruitful further algorithmic development: while DMRG is a very mature method by now, I still see potential for further improvements, as exemplified by a number of recently introduced algorithms.},
	number = {1},
	urldate = {2025-01-28},
	journal = {Annals of Physics},
	author = {Schollw√∂ck, Ulrich},
	month = jan,
	year = {2011},
	pages = {96--192},
}

@misc{mangini_low-variance_2024,
	title = {Low-variance observable estimation with informationally-complete measurements and tensor networks},
	url = {http://arxiv.org/abs/2407.02923},
	doi = {10.48550/arXiv.2407.02923},
	abstract = {We propose a method for providing unbiased estimators of multiple observables with low statistical error by utilizing informationally (over)complete measurements and tensor networks. The technique consists of an observable-specific classical optimization of the measurement data based on tensor networks leading to low-variance estimations. Compared to other observable estimation protocols based on classical shadows and measurement frames, our approach offers several advantages: (i) it can be optimized to provide lower statistical error, resulting in a reduced measurement budget to achieve a specified estimation precision; (ii) it scales to a large number of qubits due to the tensor network structure; (iii) it can be applied to any measurement protocol with measurement operators that have an efficient representation in terms of tensor networks. We benchmark the method through various numerical examples, including spin and chemical systems in both infinite and finite statistics scenarios, and show how optimal estimation can be found even when we use tensor networks with low bond dimensions.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Mangini, Stefano and Cavalcanti, Daniel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02923 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{garcia-perez_learning_2021,
	title = {Learning to {Measure}: {Adaptive} {Informationally} {Complete} {Generalized} {Measurements} for {Quantum} {Algorithms}},
	volume = {2},
	issn = {2691-3399},
	shorttitle = {Learning to {Measure}},
	url = {http://arxiv.org/abs/2104.00569},
	doi = {10.1103/PRXQuantum.2.040342},
	abstract = {Many prominent quantum computing algorithms with applications in fields such as chemistry and materials science require a large number of measurements, which represents an important roadblock for future real-world use cases. We introduce a novel approach to tackle this problem through an adaptive measurement scheme. We present an algorithm that optimizes informationally complete positive operator-valued measurements (POVMs) on the fly in order to minimize the statistical fluctuations in the estimation of relevant cost functions. We show its advantage by improving the efficiency of the variational quantum eigensolver in calculating ground-state energies of molecular Hamiltonians with extensive numerical simulations. Our results indicate that the proposed method is competitive with state-of-the-art measurement-reduction approaches in terms of efficiency. In addition, the informational completeness of the approach offers a crucial advantage, as the measurement data can be reused to infer other quantities of interest. We demonstrate the feasibility of this prospect by reusing ground-state energy-estimation data to perform high-fidelity reduced state tomography.},
	number = {4},
	urldate = {2025-01-27},
	journal = {PRX Quantum},
	author = {Garc√≠a-P√©rez, Guillermo and Rossi, Matteo A. C. and Sokolov, Boris and Tacchino, Francesco and Barkoutsos, Panagiotis Kl and Mazzola, Guglielmo and Tavernelli, Ivano and Maniscalco, Sabrina},
	month = nov,
	year = {2021},
	note = {arXiv:2104.00569 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {040342},
}

@article{han_unsupervised_2018,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012},
	doi = {10.1103/PhysRevX.8.031012},
	abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
	number = {3},
	urldate = {2025-01-24},
	journal = {Physical Review X},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = jul,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {031012},
}

@article{rudolph_trainability_2024,
	title = {Trainability barriers and opportunities in quantum generative modeling},
	volume = {10},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-024-00902-0},
	doi = {10.1038/s41534-024-00902-0},
	abstract = {Abstract
            Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {npj Quantum Information},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zo√´},
	month = nov,
	year = {2024},
	pages = {116},
}

@book{collura_tensor_2024,
	title = {Tensor {Network} {Techniques} for {Quantum} {Computation}},
	url = {http://arxiv.org/abs/2503.04423},
	abstract = {This book serves as an introductory yet thorough guide to tensor networks and their applications in quantum computation and quantum information, designed for advanced undergraduate and graduate-level readers. In Part I, foundational topics are covered, including tensor structures and network representations like Matrix Product States (MPS) and Tree Tensor Networks (TTN). These preliminaries provide readers with the core mathematical tools and concepts necessary for quantum physics and quantum computing applications, bridging the gap between multi-linear algebra and complex quantum systems. Part II explores practical applications of tensor networks in simulating quantum dynamics, with a particular focus on the efficiency they offer for systems of high computational complexity. Key topics include Hamiltonian dynamics, quantum annealing, open system dynamics, and optimization strategies using TN frameworks. A final chapter addresses the emerging role of "quantum magic" in tensor networks. It delves into non-stabilizer states and their contribution to quantum computational power beyond classical simulability, featuring methods such as stabilizer-enhanced MPS and the Clifford-dressed TDVP.},
	urldate = {2025-03-19},
	author = {Collura, Mario and Lami, Guglielmo and Ranabhat, Nishan and Santini, Alessandro},
	month = dec,
	year = {2024},
	doi = {10.22323/9788898587049},
	note = {arXiv:2503.04423 [quant-ph]},
	keywords = {Quantum Physics},
}

@misc{ben-dov_regularized_2025,
    title = {Regularized second-order optimization of tensor-network {Born} machines},
    url = {http://arxiv.org/abs/2501.18691},
    doi = {10.48550/arXiv.2501.18691},
    abstract = {Tensor-network Born machines (TNBMs) are quantum-inspired generative models for learning data distributions. Using tensor-network contraction and optimization techniques, the model learns an efficient representation of the target distribution, capable of capturing complex correlations with a compact parameterization. Despite their promise, the optimization of TNBMs presents several challenges. A key bottleneck of TNBMs is the logarithmic nature of the loss function that is commonly used for this problem. The single-tensor logarithmic optimization problem cannot be solved analytically, necessitating an iterative approach that slows down convergence and increases the risk of getting trapped in one of many non-optimal local minima. In this paper, we present an improved second-order optimization technique for TNBM training, which significantly enhances convergence rates and the quality of the optimized model. Our method employs a modified Newton's method on the manifold of normalized states, incorporating regularization of the loss landscape to mitigate local minima issues. We demonstrate the effectiveness of our approach by training a one-dimensional matrix product state (MPS) on both discrete and continuous datasets, showcasing its advantages in terms of stability, efficiency, and generalization.},
    language = {en},
    urldate = {2025-03-23},
    publisher = {arXiv},
    author = {Ben-Dov, Matan and Chen, Jing},
    month = jan,
    year = {2025},
    note = {arXiv:2501.18691 [cs]},
    keywords = {Computer Science - Machine Learning, Quantum Physics},
}
@inproceedings{glasser_expressive_2019,
    title = {Expressive power of tensor-network factorizations for probabilistic modeling},
    volume = {32},
    url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
    abstract = {Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.},
    urldate = {2025-03-11},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates, Inc.},
    author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
    year = {2019},
}
@article{haghshenas_variational_2022,
    title = {The {Variational} {Power} of {Quantum} {Circuit} {Tensor} {Networks}},
    volume = {12},
    issn = {2160-3308},
    url = {http://arxiv.org/abs/2107.01307},
    doi = {10.1103/PhysRevX.12.011047},
    abstract = {We characterize the variational power of quantum circuit tensor networks in the representation of physical many-body ground-states. Such tensor networks are formed by replacing the dense block unitaries and isometries in standard tensor networks by local quantum circuits. We explore both quantum circuit matrix product states and the quantum circuit multi-scale entanglement renormalization ansatz, and introduce an adaptive method to optimize the resulting circuits to high fidelity with more than \$10{\textasciicircum}4\$ parameters. We benchmark their expressiveness against standard tensor networks, as well as other common circuit architectures, for the 1D/2D Heisenberg and 1D Fermi-Hubbard models. We find quantum circuit tensor networks to be substantially more expressive than other quantum circuits for these problems, and that they can even be more compact than standard tensor networks. Extrapolating to circuit depths which can no longer be emulated classically, this suggests a region of advantage in quantum expressiveness in the representation of physical ground-states.},
    number = {1},
    urldate = {2025-03-23},
    journal = {Physical Review X},
    author = {Haghshenas, Reza and Gray, Johnnie and Potter, Andrew C. and Chan, Garnet Kin-Lic},
    month = mar,
    year = {2022},
    note = {arXiv:2107.01307 [quant-ph]},
    keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
    pages = {011047},
}
@article{martin_barren_2023,
    title = {Barren plateaus in quantum tensor network optimization},
    volume = {7},
    issn = {2521-327X},
    url = {http://arxiv.org/abs/2209.00292},
    doi = {10.22331/q-2023-04-13-974},
    abstract = {We analyze the barren plateau phenomenon in the variational optimization of quantum circuits inspired by matrix product states (qMPS), tree tensor networks (qTTN), and the multiscale entanglement renormalization ansatz (qMERA). We consider as the cost function the expectation value of a Hamiltonian that is a sum of local terms. For randomly chosen variational parameters we show that the variance of the cost function gradient decreases exponentially with the distance of a Hamiltonian term from the canonical centre in the quantum tensor network. Therefore, as a function of qubit count, for qMPS most gradient variances decrease exponentially and for qTTN as well as qMERA they decrease polynomially. We also show that the calculation of these gradients is exponentially more efficient on a classical computer than on a quantum computer.},
    urldate = {2025-03-24},
    journal = {Quantum},
    author = {Mart√≠n, Enrique Cervero and Plekhanov, Kirill and Lubasch, Michael},
    month = apr,
    year = {2023},
    note = {arXiv:2209.00292 [quant-ph]},
    keywords = {Quantum Physics},
    pages = {974},
}