
@article{jerbi_shadows_2024,
	title = {Shadows of quantum machine learning},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-49877-8},
	doi = {10.1038/s41467-024-49877-8},
	abstract = {Quantum machine learning is often highlighted as one of the most promising practical applications for which quantum computers could provide a computational advantage. However, a major obstacle to the widespread use of quantum machine learning models in practice is that these models, even once trained, still require access to a quantum computer in order to be evaluated on new data. To solve this issue, we introduce a class of quantum models where quantum resources are only required during training, while the deployment of the trained model is classical. Specifically, the training phase of our models ends with the generation of a ‚Äòshadow model‚Äô from which the classical deployment becomes possible. We prove that: (i) this class of models is universal for classically-deployed quantum machine learning; (ii) it does have restricted learning capacities compared to ‚Äòfully quantum‚Äô models, but nonetheless (iii) it achieves a provable learning advantage over fully classical learners, contingent on widely believed assumptions in complexity theory. These results provide compelling evidence that quantum machine learning can confer learning advantages across a substantially broader range of scenarios, where quantum computers are exclusively employed during the training phase. By enabling classical deployment, our approach facilitates the implementation of quantum machine learning models in various practical contexts.},
	language = {en},
	number = {1},
	urldate = {2024-10-29},
	journal = {Nat Commun},
	author = {Jerbi, Sofiene and Gyurik, Casper and Marshall, Simon C. and Molteni, Riccardo and Dunjko, Vedran},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information theory and computation, Quantum information},
	pages = {5676},
}

@misc{rudolph_generation_2022,
	title = {Generation of {High}-{Resolution} {Handwritten} {Digits} with an {Ion}-{Trap} {Quantum} {Computer}},
	url = {http://arxiv.org/abs/2012.03924},
	doi = {10.48550/arXiv.2012.03924},
	abstract = {Generating high-quality data (e.g. images or video) is one of the most exciting and challenging frontiers in unsupervised machine learning. Utilizing quantum computers in such tasks to potentially enhance conventional machine learning algorithms has emerged as a promising application, but poses big challenges due to the limited number of qubits and the level of gate noise in available devices. In this work, we provide the first practical and experimental implementation of a quantum-classical generative algorithm capable of generating high-resolution images of handwritten digits with state-of-the-art gate-based quantum computers. In our quantum-assisted machine learning framework, we implement a quantum-circuit based generative model to learn and sample the prior distribution of a Generative Adversarial Network. We introduce a multi-basis technique that leverages the unique possibility of measuring quantum states in different bases, hence enhancing the expressivity of the prior distribution. We train this hybrid algorithm on an ion-trap device based on \${\textasciicircum}\{171\}\$Yb\${\textasciicircum}\{+\}\$ ion qubits to generate high-quality images and quantitatively outperform comparable classical Generative Adversarial Networks trained on the popular MNIST data set for handwritten digits.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Rudolph, Manuel S. and Toussaint, Ntwali Bashige and Katabarwa, Amara and Johri, Sonika and Peropadre, Borja and Perdomo-Ortiz, Alejandro},
	month = jun,
	year = {2022},
	note = {arXiv:2012.03924},
	keywords = {Quantum Physics},
}

@misc{mangini_low-variance_2024,
	title = {Low-variance observable estimation with informationally-complete measurements and tensor networks},
	url = {http://arxiv.org/abs/2407.02923},
	doi = {10.48550/arXiv.2407.02923},
	abstract = {We propose a method for providing unbiased estimators of multiple observables with low statistical error by utilizing informationally (over)complete measurements and tensor networks. The technique consists of an observable-specific classical optimization of the measurement data based on tensor networks leading to low-variance estimations. Compared to other observable estimation protocols based on classical shadows and measurement frames, our approach offers several advantages: (i) it can be optimized to provide lower statistical error, resulting in a reduced measurement budget to achieve a specified estimation precision; (ii) it scales to a large number of qubits due to the tensor network structure; (iii) it can be applied to any measurement protocol with measurement operators that have an efficient representation in terms of tensor networks. We benchmark the method through various numerical examples, including spin and chemical systems in both infinite and finite statistics scenarios, and show how optimal estimation can be found even when we use tensor networks with low bond dimensions.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Mangini, Stefano and Cavalcanti, Daniel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02923 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{garcia-perez_learning_2021,
	title = {Learning to {Measure}: {Adaptive} {Informationally} {Complete} {Generalized} {Measurements} for {Quantum} {Algorithms}},
	volume = {2},
	issn = {2691-3399},
	shorttitle = {Learning to {Measure}},
	url = {http://arxiv.org/abs/2104.00569},
	doi = {10.1103/PRXQuantum.2.040342},
	abstract = {Many prominent quantum computing algorithms with applications in fields such as chemistry and materials science require a large number of measurements, which represents an important roadblock for future real-world use cases. We introduce a novel approach to tackle this problem through an adaptive measurement scheme. We present an algorithm that optimizes informationally complete positive operator-valued measurements (POVMs) on the fly in order to minimize the statistical fluctuations in the estimation of relevant cost functions. We show its advantage by improving the efficiency of the variational quantum eigensolver in calculating ground-state energies of molecular Hamiltonians with extensive numerical simulations. Our results indicate that the proposed method is competitive with state-of-the-art measurement-reduction approaches in terms of efficiency. In addition, the informational completeness of the approach offers a crucial advantage, as the measurement data can be reused to infer other quantities of interest. We demonstrate the feasibility of this prospect by reusing ground-state energy-estimation data to perform high-fidelity reduced state tomography.},
	number = {4},
	urldate = {2025-01-27},
	journal = {PRX Quantum},
	author = {Garc√≠a-P√©rez, Guillermo and Rossi, Matteo A. C. and Sokolov, Boris and Tacchino, Francesco and Barkoutsos, Panagiotis Kl and Mazzola, Guglielmo and Tavernelli, Ivano and Maniscalco, Sabrina},
	month = nov,
	year = {2021},
	note = {arXiv:2104.00569 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {040342},
}

@article{han_unsupervised_2018,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012},
	doi = {10.1103/PhysRevX.8.031012},
	abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
	number = {3},
	urldate = {2025-01-24},
	journal = {Phys. Rev. X},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = jul,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {031012},
}

@article{cheng_tree_2019,
	title = {Tree tensor networks for generative modeling},
	volume = {99},
	issn = {2469-9950, 2469-9969},
	url = {https://link.aps.org/doi/10.1103/PhysRevB.99.155131},
	doi = {10.1103/PhysRevB.99.155131},
	language = {en},
	number = {15},
	urldate = {2025-01-20},
	journal = {Phys. Rev. B},
	author = {Cheng, Song and Wang, Lei and Xiang, Tao and Zhang, Pan},
	month = apr,
	year = {2019},
	pages = {155131},
}

@misc{meiburg_generative_2024,
	title = {Generative {Learning} of {Continuous} {Data} by {Tensor} {Networks}},
	url = {http://arxiv.org/abs/2310.20498},
	doi = {10.48550/arXiv.2310.20498},
	abstract = {Beyond their origin in modeling many-body quantum systems, tensor networks have emerged as a promising class of models for solving machine learning problems, notably in unsupervised generative learning. While possessing many desirable features arising from their quantum-inspired nature, tensor network generative models have previously been largely restricted to binary or categorical data, limiting their utility in real-world modeling problems. We overcome this by introducing a new family of tensor network generative models for continuous data, which are capable of learning from distributions containing continuous random variables. We develop our method in the setting of matrix product states, first deriving a universal expressivity theorem proving the ability of this model family to approximate any reasonably smooth probability density function with arbitrary precision. We then benchmark the performance of this model on several synthetic and real-world datasets, finding that the model learns and generalizes well on distributions of continuous and discrete variables. We develop methods for modeling different data domains, and introduce a trainable compression layer which is found to increase model performance given limited memory or computational resources. Overall, our methods give important theoretical and empirical evidence of the efficacy of quantum-inspired methods for the rapidly growing field of generative learning.},
	urldate = {2025-02-14},
	publisher = {arXiv},
	author = {Meiburg, Alex and Chen, Jing and Miller, Jacob and Tihon, Rapha√´lle and Rabusseau, Guillaume and Perdomo-Ortiz, Alejandro},
	month = jul,
	year = {2024},
	note = {arXiv:2310.20498 [cs]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Quantum Physics, Statistics - Machine Learning},
}

@article{rudolph_trainability_2024,
	title = {Trainability barriers and opportunities in quantum generative modeling},
	volume = {10},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-024-00902-0},
	doi = {10.1038/s41534-024-00902-0},
	abstract = {Abstract
            Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {npj Quantum Inf},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zo√´},
	month = nov,
	year = {2024},
	pages = {116},
}

@inproceedings{glasser_expressive_2019,
	title = {Expressive power of tensor-network factorizations for probabilistic modeling},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
	abstract = {Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.},
	urldate = {2025-03-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
	year = {2019},
}

@article{martin_barren_2023,
	title = {Barren plateaus in quantum tensor network optimization},
	volume = {7},
	issn = {2521-327X},
	url = {http://arxiv.org/abs/2209.00292},
	doi = {10.22331/q-2023-04-13-974},
	abstract = {We analyze the barren plateau phenomenon in the variational optimization of quantum circuits inspired by matrix product states (qMPS), tree tensor networks (qTTN), and the multiscale entanglement renormalization ansatz (qMERA). We consider as the cost function the expectation value of a Hamiltonian that is a sum of local terms. For randomly chosen variational parameters we show that the variance of the cost function gradient decreases exponentially with the distance of a Hamiltonian term from the canonical centre in the quantum tensor network. Therefore, as a function of qubit count, for qMPS most gradient variances decrease exponentially and for qTTN as well as qMERA they decrease polynomially. We also show that the calculation of these gradients is exponentially more efficient on a classical computer than on a quantum computer.},
	urldate = {2025-03-24},
	journal = {Quantum},
	author = {Mart√≠n, Enrique Cervero and Plekhanov, Kirill and Lubasch, Michael},
	month = apr,
	year = {2023},
	note = {arXiv:2209.00292 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {974},
}

@article{levy_classical_2024,
	title = {Classical shadows for quantum process tomography on near-term quantum computers},
	volume = {6},
	url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.013029},
	doi = {10.1103/PhysRevResearch.6.013029},
	abstract = {Quantum process tomography is a powerful tool for understanding quantum channels and characterizing the properties of quantum devices. Inspired by recent advances using classical shadows in quantum state tomography [H.-Y. Huang, R. Kueng, and J. Preskill, Nat. Phys. 16, 1050 (2020).], we have developed ShadowQPT, a classical shadow method for quantum process tomography. We introduce two related formulations with and without ancilla qubits. ShadowQPT stochastically reconstructs the Choi matrix of the device allowing for an a posteri classical evaluation of the device on arbitrary inputs with respect to arbitrary outputs. Using shadows, we then show how to compute overlaps, generate all ùëò-weight reduced processes, and perform reconstruction via Hamiltonian learning. These latter two tasks are efficient for large systems as the number of quantum measurements needed scales only logarithmically with the number of qubits. A number of additional approximations and improvements are developed, including the use of a pair-factorized Clifford shadow and a series of postprocessing techniques that significantly enhance the accuracy for recovering the quantum channel. We have implemented ShadowQPT using both Pauli and Clifford measurements on the IonQ trapped ion quantum computer for quantum processes up to ùëõ=4 qubits, and we achieved good performance.},
	number = {1},
	urldate = {2025-04-13},
	journal = {Phys. Rev. Res.},
	author = {Levy, Ryan and Luo, Di and Clark, Bryan K.},
	month = jan,
	year = {2024},
	note = {Publisher: American Physical Society},
	pages = {013029},
}
