
@misc{rudolph_generation_2022,
	title = {Generation of {High}-{Resolution} {Handwritten} {Digits} with an {Ion}-{Trap} {Quantum} {Computer}},
	url = {http://arxiv.org/abs/2012.03924},
	doi = {10.48550/arXiv.2012.03924},
	abstract = {Generating high-quality data (e.g. images or video) is one of the most exciting and challenging frontiers in unsupervised machine learning. Utilizing quantum computers in such tasks to potentially enhance conventional machine learning algorithms has emerged as a promising application, but poses big challenges due to the limited number of qubits and the level of gate noise in available devices. In this work, we provide the first practical and experimental implementation of a quantum-classical generative algorithm capable of generating high-resolution images of handwritten digits with state-of-the-art gate-based quantum computers. In our quantum-assisted machine learning framework, we implement a quantum-circuit based generative model to learn and sample the prior distribution of a Generative Adversarial Network. We introduce a multi-basis technique that leverages the unique possibility of measuring quantum states in different bases, hence enhancing the expressivity of the prior distribution. We train this hybrid algorithm on an ion-trap device based on \${\textasciicircum}\{171\}\$Yb\${\textasciicircum}\{+\}\$ ion qubits to generate high-quality images and quantitatively outperform comparable classical Generative Adversarial Networks trained on the popular MNIST data set for handwritten digits.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Rudolph, Manuel S. and Toussaint, Ntwali Bashige and Katabarwa, Amara and Johri, Sonika and Peropadre, Borja and Perdomo-Ortiz, Alejandro},
	month = jun,
	year = {2022},
	note = {arXiv:2012.03924},
	keywords = {Quantum Physics},
}

@article{schollwock_density-matrix_2011,
	series = {January 2011 {Special} {Issue}},
	title = {The density-matrix renormalization group in the age of matrix product states},
	volume = {326},
	issn = {0003-4916},
	url = {https://www.sciencedirect.com/science/article/pii/S0003491610001752},
	doi = {10.1016/j.aop.2010.09.012},
	abstract = {The density-matrix renormalization group method (DMRG) has established itself over the last decade as the leading method for the simulation of the statics and dynamics of one-dimensional strongly correlated quantum lattice systems. In the further development of the method, the realization that DMRG operates on a highly interesting class of quantum states, so-called matrix product states (MPS), has allowed a much deeper understanding of the inner structure of the DMRG method, its further potential and its limitations. In this paper, I want to give a detailed exposition of current DMRG thinking in the MPS language in order to make the advisable implementation of the family of DMRG algorithms in exclusively MPS terms transparent. I then move on to discuss some directions of potentially fruitful further algorithmic development: while DMRG is a very mature method by now, I still see potential for further improvements, as exemplified by a number of recently introduced algorithms.},
	number = {1},
	urldate = {2025-01-28},
	journal = {Annals of Physics},
	author = {Schollwöck, Ulrich},
	month = jan,
	year = {2011},
	pages = {96--192},
}

@misc{mangini_low-variance_2024,
	title = {Low-variance observable estimation with informationally-complete measurements and tensor networks},
	url = {http://arxiv.org/abs/2407.02923},
	doi = {10.48550/arXiv.2407.02923},
	abstract = {We propose a method for providing unbiased estimators of multiple observables with low statistical error by utilizing informationally (over)complete measurements and tensor networks. The technique consists of an observable-specific classical optimization of the measurement data based on tensor networks leading to low-variance estimations. Compared to other observable estimation protocols based on classical shadows and measurement frames, our approach offers several advantages: (i) it can be optimized to provide lower statistical error, resulting in a reduced measurement budget to achieve a specified estimation precision; (ii) it scales to a large number of qubits due to the tensor network structure; (iii) it can be applied to any measurement protocol with measurement operators that have an efficient representation in terms of tensor networks. We benchmark the method through various numerical examples, including spin and chemical systems in both infinite and finite statistics scenarios, and show how optimal estimation can be found even when we use tensor networks with low bond dimensions.},
	urldate = {2025-01-27},
	publisher = {arXiv},
	author = {Mangini, Stefano and Cavalcanti, Daniel},
	month = jul,
	year = {2024},
	note = {arXiv:2407.02923 [quant-ph]},
	keywords = {Quantum Physics},
}

@article{garcia-perez_learning_2021,
	title = {Learning to {Measure}: {Adaptive} {Informationally} {Complete} {Generalized} {Measurements} for {Quantum} {Algorithms}},
	volume = {2},
	issn = {2691-3399},
	shorttitle = {Learning to {Measure}},
	url = {http://arxiv.org/abs/2104.00569},
	doi = {10.1103/PRXQuantum.2.040342},
	abstract = {Many prominent quantum computing algorithms with applications in fields such as chemistry and materials science require a large number of measurements, which represents an important roadblock for future real-world use cases. We introduce a novel approach to tackle this problem through an adaptive measurement scheme. We present an algorithm that optimizes informationally complete positive operator-valued measurements (POVMs) on the fly in order to minimize the statistical fluctuations in the estimation of relevant cost functions. We show its advantage by improving the efficiency of the variational quantum eigensolver in calculating ground-state energies of molecular Hamiltonians with extensive numerical simulations. Our results indicate that the proposed method is competitive with state-of-the-art measurement-reduction approaches in terms of efficiency. In addition, the informational completeness of the approach offers a crucial advantage, as the measurement data can be reused to infer other quantities of interest. We demonstrate the feasibility of this prospect by reusing ground-state energy-estimation data to perform high-fidelity reduced state tomography.},
	number = {4},
	urldate = {2025-01-27},
	journal = {PRX Quantum},
	author = {García-Pérez, Guillermo and Rossi, Matteo A. C. and Sokolov, Boris and Tacchino, Francesco and Barkoutsos, Panagiotis Kl and Mazzola, Guglielmo and Tavernelli, Ivano and Maniscalco, Sabrina},
	month = nov,
	year = {2021},
	note = {arXiv:2104.00569 [quant-ph]},
	keywords = {Quantum Physics},
	pages = {040342},
}

@article{han_unsupervised_2018,
	title = {Unsupervised {Generative} {Modeling} {Using} {Matrix} {Product} {States}},
	volume = {8},
	url = {https://link.aps.org/doi/10.1103/PhysRevX.8.031012},
	doi = {10.1103/PhysRevX.8.031012},
	abstract = {Generative modeling, which learns joint probability distribution from data and generates samples according to it, is an important task in machine learning and artificial intelligence. Inspired by probabilistic interpretation of quantum physics, we propose a generative model using matrix product states, which is a tensor network originally proposed for describing (particularly one-dimensional) entangled quantum states. Our model enjoys efficient learning analogous to the density matrix renormalization group method, which allows dynamically adjusting dimensions of the tensors and offers an efficient direct sampling approach for generative tasks. We apply our method to generative modeling of several standard data sets including the Bars and Stripes random binary patterns and the MNIST handwritten digits to illustrate the abilities, features, and drawbacks of our model over popular generative models such as the Hopfield model, Boltzmann machines, and generative adversarial networks. Our work sheds light on many interesting directions of future exploration in the development of quantum-inspired algorithms for unsupervised machine learning, which are promisingly possible to realize on quantum devices.},
	number = {3},
	urldate = {2025-01-24},
	journal = {Physical Review X},
	author = {Han, Zhao-Yu and Wang, Jun and Fan, Heng and Wang, Lei and Zhang, Pan},
	month = jul,
	year = {2018},
	note = {Publisher: American Physical Society},
	pages = {031012},
}

@article{rudolph_trainability_2024,
	title = {Trainability barriers and opportunities in quantum generative modeling},
	volume = {10},
	issn = {2056-6387},
	url = {https://www.nature.com/articles/s41534-024-00902-0},
	doi = {10.1038/s41534-024-00902-0},
	abstract = {Abstract
            Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	language = {en},
	number = {1},
	urldate = {2025-02-24},
	journal = {npj Quantum Information},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zoë},
	month = nov,
	year = {2024},
	pages = {116},
}

@book{collura_tensor_2024,
	title = {Tensor {Network} {Techniques} for {Quantum} {Computation}},
	url = {http://arxiv.org/abs/2503.04423},
	abstract = {This book serves as an introductory yet thorough guide to tensor networks and their applications in quantum computation and quantum information, designed for advanced undergraduate and graduate-level readers. In Part I, foundational topics are covered, including tensor structures and network representations like Matrix Product States (MPS) and Tree Tensor Networks (TTN). These preliminaries provide readers with the core mathematical tools and concepts necessary for quantum physics and quantum computing applications, bridging the gap between multi-linear algebra and complex quantum systems. Part II explores practical applications of tensor networks in simulating quantum dynamics, with a particular focus on the efficiency they offer for systems of high computational complexity. Key topics include Hamiltonian dynamics, quantum annealing, open system dynamics, and optimization strategies using TN frameworks. A final chapter addresses the emerging role of "quantum magic" in tensor networks. It delves into non-stabilizer states and their contribution to quantum computational power beyond classical simulability, featuring methods such as stabilizer-enhanced MPS and the Clifford-dressed TDVP.},
	urldate = {2025-03-19},
	author = {Collura, Mario and Lami, Guglielmo and Ranabhat, Nishan and Santini, Alessandro},
	month = dec,
	year = {2024},
	doi = {10.22323/9788898587049},
	note = {arXiv:2503.04423 [quant-ph]},
	keywords = {Quantum Physics},
}

@misc{ben-dov_regularized_2025,
    title = {Regularized second-order optimization of tensor-network {Born} machines},
    url = {http://arxiv.org/abs/2501.18691},
    doi = {10.48550/arXiv.2501.18691},
    abstract = {Tensor-network Born machines (TNBMs) are quantum-inspired generative models for learning data distributions. Using tensor-network contraction and optimization techniques, the model learns an efficient representation of the target distribution, capable of capturing complex correlations with a compact parameterization. Despite their promise, the optimization of TNBMs presents several challenges. A key bottleneck of TNBMs is the logarithmic nature of the loss function that is commonly used for this problem. The single-tensor logarithmic optimization problem cannot be solved analytically, necessitating an iterative approach that slows down convergence and increases the risk of getting trapped in one of many non-optimal local minima. In this paper, we present an improved second-order optimization technique for TNBM training, which significantly enhances convergence rates and the quality of the optimized model. Our method employs a modified Newton's method on the manifold of normalized states, incorporating regularization of the loss landscape to mitigate local minima issues. We demonstrate the effectiveness of our approach by training a one-dimensional matrix product state (MPS) on both discrete and continuous datasets, showcasing its advantages in terms of stability, efficiency, and generalization.},
    language = {en},
    urldate = {2025-03-23},
    publisher = {arXiv},
    author = {Ben-Dov, Matan and Chen, Jing},
    month = jan,
    year = {2025},
    note = {arXiv:2501.18691 [cs]},
    keywords = {Computer Science - Machine Learning, Quantum Physics},
}
@inproceedings{glasser_expressive_2019,
    title = {Expressive power of tensor-network factorizations for probabilistic modeling},
    volume = {32},
    url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/b86e8d03fe992d1b0e19656875ee557c-Abstract.html},
    abstract = {Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.},
    urldate = {2025-03-11},
    booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
    publisher = {Curran Associates, Inc.},
    author = {Glasser, Ivan and Sweke, Ryan and Pancotti, Nicola and Eisert, Jens and Cirac, Ignacio},
    year = {2019},
}
@article{haghshenas_variational_2022,
    title = {The {Variational} {Power} of {Quantum} {Circuit} {Tensor} {Networks}},
    volume = {12},
    issn = {2160-3308},
    url = {http://arxiv.org/abs/2107.01307},
    doi = {10.1103/PhysRevX.12.011047},
    abstract = {We characterize the variational power of quantum circuit tensor networks in the representation of physical many-body ground-states. Such tensor networks are formed by replacing the dense block unitaries and isometries in standard tensor networks by local quantum circuits. We explore both quantum circuit matrix product states and the quantum circuit multi-scale entanglement renormalization ansatz, and introduce an adaptive method to optimize the resulting circuits to high fidelity with more than \$10{\textasciicircum}4\$ parameters. We benchmark their expressiveness against standard tensor networks, as well as other common circuit architectures, for the 1D/2D Heisenberg and 1D Fermi-Hubbard models. We find quantum circuit tensor networks to be substantially more expressive than other quantum circuits for these problems, and that they can even be more compact than standard tensor networks. Extrapolating to circuit depths which can no longer be emulated classically, this suggests a region of advantage in quantum expressiveness in the representation of physical ground-states.},
    number = {1},
    urldate = {2025-03-23},
    journal = {Physical Review X},
    author = {Haghshenas, Reza and Gray, Johnnie and Potter, Andrew C. and Chan, Garnet Kin-Lic},
    month = mar,
    year = {2022},
    note = {arXiv:2107.01307 [quant-ph]},
    keywords = {Condensed Matter - Strongly Correlated Electrons, Quantum Physics},
    pages = {011047},
}
@article{martin_barren_2023,
    title = {Barren plateaus in quantum tensor network optimization},
    volume = {7},
    issn = {2521-327X},
    url = {http://arxiv.org/abs/2209.00292},
    doi = {10.22331/q-2023-04-13-974},
    abstract = {We analyze the barren plateau phenomenon in the variational optimization of quantum circuits inspired by matrix product states (qMPS), tree tensor networks (qTTN), and the multiscale entanglement renormalization ansatz (qMERA). We consider as the cost function the expectation value of a Hamiltonian that is a sum of local terms. For randomly chosen variational parameters we show that the variance of the cost function gradient decreases exponentially with the distance of a Hamiltonian term from the canonical centre in the quantum tensor network. Therefore, as a function of qubit count, for qMPS most gradient variances decrease exponentially and for qTTN as well as qMERA they decrease polynomially. We also show that the calculation of these gradients is exponentially more efficient on a classical computer than on a quantum computer.},
    urldate = {2025-03-24},
    journal = {Quantum},
    author = {Martín, Enrique Cervero and Plekhanov, Kirill and Lubasch, Michael},
    month = apr,
    year = {2023},
    note = {arXiv:2209.00292 [quant-ph]},
    keywords = {Quantum Physics},
    pages = {974},
}