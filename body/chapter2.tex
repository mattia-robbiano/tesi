\chapter{The basics}

\section{Introduction to Quantum Computing}

\lipsum[1-2] 
\cite{rudolph_generation_2022}
\cite{schollwock_density-matrix_2011}
\cite{mangini_low-variance_2024}
\cite{rudolph_trainability_2024}
\cite{collura_tensor_2024}
\cite{garcia-perez_learning_2021}

\section{Introduction to Tensor Network}

\lipsum[1-2]

\section{Introduction to Machine Learning and Generative Models}

Generative models are a class of machine learning models designed to learn the underlying probability distribution of a given dataset and generate new samples that are statistically similar to those in the training data. A fundamental distinction in generative modeling is between explicit and implicit models, which differ in their approach to defining and optimizing the learned probability distribution.

Explicit models define an explicit probability distribution that can be directly evaluated, allowing for gradient-based optimization techniques that leverage likelihood-based loss functions such as the Kullback-Leibler (KL) divergence. Implicit models, on the other hand, do not require an explicit formulation of the probability distribution. Instead, they generate samples and optimize the model by comparing distributions through statistical measures like the Maximum Mean Discrepancy (MMD). In this chapter, we explore these two paradigms in the context of generative models for quantum states, highlighting their theoretical properties and implications for training efficiency.
