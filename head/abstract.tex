\chapter*{Abstract}
Generative models based on tensor networks offer a promising framework for capturing complex correlations in high-dimensional data. In this work, we propose a novel approach to generative modeling through global training of tensor network architectures, where every tensor element is optimized as a trainable parameter. Our method is evaluated on multiple datasets, and we consider two complementary loss functions: one rooted in explicit formulations and another in implicit ones. In particular, we reformulate the Maximum Mean Discrepancy (MMD) loss, typically an implicit method, into an explicit version via our tensor network representation—a strategy that aligns well with recent findings \cite{rudolph_trainability_2024}. Additionally, we incorporate advanced measurement techniques using Positive Operator-Valued Measures (POVMs) to further enhance the model’s capacity to represent intricate probability distributions. All experiments are implemented using Quimb and JAX. Our study provides insights into the trade-offs between implicit and explicit training approaches in quantum generative modeling and highlights the potential benefits of a unified tensor network framework.

