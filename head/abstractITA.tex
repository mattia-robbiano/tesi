\chapter*{Italian abstract}
I modelli generativi basati su tensor network offrono un quadro promettente per la cattura di correlazioni complesse in dati ad alta dimensionalità. In questo lavoro, proponiamo un approccio innovativo alla generazione di dati attraverso l’addestramento globale di tensor network, in cui ogni elemento della rete è trattato come parametro ottimizzabile. Il nostro metodo viene testato su diversi dataset e analizziamo due funzioni di costo complementari: una formulata esplicitamente e una di natura implicita. In particolare, riformuliamo la funzione di costo Maximum Mean Discrepancy (MMD), solitamente implicita, in una versione esplicita sfruttando la rappresentazione mediante tensor network, in accordo con risultati recenti \cite{rudolph_trainability_2024}. Inoltre, utilizziamo misurazioni avanzate basate su Positive Operator-Valued Measures (POVMs) per migliorare la capacità del modello di rappresentare distribuzioni di probabilità complesse. Tutti gli esperimenti sono implementati con Quimb e JAX. Questo studio offre un’analisi del compromesso tra approcci espliciti e impliciti nell’addestramento di modelli generativi quantistici e evidenzia i vantaggi di una formulazione unificata basata su reti tensoriali.
